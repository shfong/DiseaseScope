{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks downloads genesets as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndex2\n",
    "import requests\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Wikipathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"http://public.ndexbio.org/v2/networkset/453c1c63-5c10-11e9-9f06-0ac135e8bacf\")\n",
    "tmp = json.loads(r.content.decode(\"utf-8\"))\n",
    "wikipathway_networks = tmp['networks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(uuid): \n",
    "    n = ndex2.create_nice_cx_from_server(\"http://public.ndexbio.org\", uuid=uuid)\n",
    "    dG = n.to_networkx()\n",
    "    \n",
    "    genes = [attrs['name'] for node, attrs in dG.node.items() \\\n",
    "                if (attrs['Type'] == 'GeneProduct') and (attrs['__gpml:XrefDatasource'] in ['Ensembl',  'Entrez Gene'])]\n",
    "    \n",
    "    name_array = dG.name.split(' - ')\n",
    "    name = name_array[1].strip() + f\" ({name_array[0].strip()})\"\n",
    "\n",
    "    return (name, genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = ndex2.create_nice_cx_from_server(\"http://public.ndexbio.org\", uuid=\"cbcce7eb-7785-11e9-848d-0ac135e8bacf\")\n",
    "# dG = n.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(16)\n",
    "result = pool.map(get_nodes, wikipathway_networks)\n",
    "wikipathway_nodes = dict(result)\n",
    "\n",
    "wikipathway_nodes = {\n",
    "    k:[i.strip().replace('\\n', '') for i in v] for k,v in wikipathway_nodes.items() \\\n",
    "        if len(v) >= 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genesets/wikipathways.txt\", \"w\") as f: \n",
    "    for name, genelist in wikipathway_nodes.items(): \n",
    "        line = f\"{name};{','.join(genelist)}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Cancer Hallmarks from msigdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallmarks='''HALLMARK_ADIPOGENESIS\n",
    "HALLMARK_ALLOGRAFT_REJECTION\n",
    "HALLMARK_ANDROGEN_RESPONSE\n",
    "HALLMARK_ANGIOGENESIS\n",
    "HALLMARK_APICAL_JUNCTION\n",
    "HALLMARK_APICAL_SURFACE\n",
    "HALLMARK_APOPTOSIS\n",
    "HALLMARK_BILE_ACID_METABOLISM\n",
    "HALLMARK_CHOLESTEROL_HOMEOSTASIS\n",
    "HALLMARK_COAGULATION\n",
    "HALLMARK_COMPLEMENT\n",
    "HALLMARK_DNA_REPAIR\n",
    "HALLMARK_E2F_TARGETS\n",
    "HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION\n",
    "HALLMARK_ESTROGEN_RESPONSE_EARLY\n",
    "HALLMARK_ESTROGEN_RESPONSE_LATE\n",
    "HALLMARK_FATTY_ACID_METABOLISM\n",
    "HALLMARK_G2M_CHECKPOINT\n",
    "HALLMARK_GLYCOLYSIS\n",
    "HALLMARK_HEDGEHOG_SIGNALING\n",
    "HALLMARK_HEME_METABOLISM\n",
    "HALLMARK_HYPOXIA\n",
    "HALLMARK_IL2_STAT5_SIGNALING\n",
    "HALLMARK_IL6_JAK_STAT3_SIGNALING\n",
    "HALLMARK_INFLAMMATORY_RESPONSE\n",
    "HALLMARK_INTERFERON_ALPHA_RESPONSE\n",
    "HALLMARK_INTERFERON_GAMMA_RESPONSE\n",
    "HALLMARK_KRAS_SIGNALING_DN\n",
    "HALLMARK_KRAS_SIGNALING_UP\n",
    "HALLMARK_MITOTIC_SPINDLE\n",
    "HALLMARK_MTORC1_SIGNALING\n",
    "HALLMARK_MYC_TARGETS_V1\n",
    "HALLMARK_MYC_TARGETS_V2\n",
    "HALLMARK_MYOGENESIS\n",
    "HALLMARK_NOTCH_SIGNALING\n",
    "HALLMARK_OXIDATIVE_PHOSPHORYLATION\n",
    "HALLMARK_P53_PATHWAY\n",
    "HALLMARK_PANCREAS_BETA_CELLS\n",
    "HALLMARK_PEROXISOME\n",
    "HALLMARK_PI3K_AKT_MTOR_SIGNALING\n",
    "HALLMARK_PROTEIN_SECRETION\n",
    "HALLMARK_REACTIVE_OXIGEN_SPECIES_PATHWAY\n",
    "HALLMARK_SPERMATOGENESIS\n",
    "HALLMARK_TGF_BETA_SIGNALING\n",
    "HALLMARK_TNFA_SIGNALING_VIA_NFKB\n",
    "HALLMARK_UNFOLDED_PROTEIN_RESPONSE\n",
    "HALLMARK_UV_RESPONSE_DN\n",
    "HALLMARK_UV_RESPONSE_UP\n",
    "HALLMARK_WNT_BETA_CATENIN_SIGNALING\n",
    "HALLMARK_XENOBIOTIC_METABOLISM'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cancer_hallmarks(name):\n",
    "    address = lambda x:f\"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName={x}&fileType=txt\"\n",
    "    response =(requests\n",
    "         .get(address(name))\n",
    "         .content\n",
    "         .decode('utf-8')\n",
    "         .split('\\n'))\n",
    "    \n",
    "    name = response[0].replace('_', ' ').title()\n",
    "    geneset = response[2:]\n",
    "    \n",
    "    return (name, geneset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(16)\n",
    "out = pool.map(get_cancer_hallmarks, hallmarks.split('\\n'))\n",
    "msigdb_hallmarks = dict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genesets/msigdb_hallmarks.txt\", \"w\") as f: \n",
    "    for name, geneset in msigdb_hallmarks.items(): \n",
    "        line = f\"{name};{','.join(geneset)}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CORUM complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://mips.helmholtz-muenchen.de/corum/download/allComplexes.txt.zip\", stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "corum = pd.read_csv(z.open(\"allComplexes.txt\"), sep='\\t', index_col='ComplexID')\n",
    "\n",
    "corum_map = corum[['ComplexName', 'subunits(Gene name)']].set_index('ComplexName').to_dict()['subunits(Gene name)']\n",
    "corum_map = {k: v.split(';') for k,v in corum_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genesets/corum.txt\", \"w\") as f: \n",
    "    for name, geneset in corum_map.items(): \n",
    "        line = f\"{name};{','.join(geneset)}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining geneset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genesets(file): \n",
    "    with open(file) as f: \n",
    "        geneset = {}\n",
    "        for line in f.readlines(): \n",
    "            line = line.strip()\n",
    "            if line: \n",
    "                arr = line.split(';')\n",
    "                genes = arr[1].split(',')\n",
    "                geneset[arr[0]] = genes\n",
    "            \n",
    "    return geneset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "msigdb = parse_genesets('genesets/msigdb_hallmarks.txt')\n",
    "wikipathways = parse_genesets('genesets/wikipathways.txt')\n",
    "corum = parse_genesets('genesets/corum.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "msigdb.update(corum)\n",
    "msigdb.update(wikipathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genesets/combined_genesets.txt\", \"w\") as f: \n",
    "    for k, v in msigdb.items(): \n",
    "        line = f\"{k};{','.join(v)}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Disease Ontology .obo file\n",
    "# Expect that the ID and name follows this pattern\n",
    "# [Term] on line n\n",
    "# id: on line n + 1\n",
    "# name: on line n + 2\n",
    "# If this expectation is not True, a ValueError will be raised\n",
    "\n",
    "r = requests.get(\"https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/master/src/ontology/HumanDO.obo\")\n",
    "\n",
    "found = False\n",
    "found_line = None\n",
    "doid = None\n",
    "name = None\n",
    "\n",
    "header = []\n",
    "\n",
    "line_count = 0\n",
    "\n",
    "results = []\n",
    "for line in r.content.decode('utf-8').split('\\n'): \n",
    "    line = line.strip()\n",
    "    line_count += 1\n",
    "\n",
    "    if line_count <= 3: \n",
    "        header.append(line)\n",
    "\n",
    "    if line == '[Term]': \n",
    "        found = True\n",
    "        found_line = line_count \n",
    "\n",
    "    elif found and line[:3] == 'id:': \n",
    "        doid = line.split('id:')[1].strip()\n",
    "        if line_count - found_line != 1: \n",
    "            raise ValueError(f\"Expected line {found_line + 1}, but line {line_count} found.\")\n",
    "\n",
    "    elif found and line[:5] == \"name:\": \n",
    "        name = line.split(\"name:\")[1].strip()\n",
    "        if line_count - found_line != 2: \n",
    "            raise ValueError(f\"Expected line {found_line + 2}, but line {line_count} found.\")\n",
    "\n",
    "        results.append((doid, name))\n",
    "        found = False\n",
    "        found_line = None\n",
    "        doid = None\n",
    "        name = None\n",
    "            \n",
    "doid_name_mapping = dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doid/doid_name_mappings.txt\", \"w\") as f: \n",
    "    for line in header: \n",
    "        line = f\"#{line}\\n\"\n",
    "        f.write(line) \n",
    "        \n",
    "    for i,j in doid_name_mapping.items():\n",
    "        line = f\"{i}%{j}\\n\"\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scope",
   "language": "python",
   "name": "scope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
